{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction tutorial\n",
    "\n",
    "This tutorial is an extended version of the quickstart quide in the README and launches the marginalisation script from the Jupyter notebook interface.\n",
    "\n",
    "We will assume that you have cloned the repo, created the conda environment from the `environment.yml` file and created your `config_local.ini`.\n",
    "\n",
    "## Input parameters\n",
    "\n",
    "### Input and output data paths\n",
    "\n",
    "To be able to do our example run on the provided data, you need to point the local configfile to your local repository clone:\n",
    "```ini\n",
    "[data_paths]\n",
    "local_path = /Users/<YourUser>/repos/ExoTiC-ISM\n",
    "```\n",
    "\n",
    "To run the script on the provided example data on the repository, you can keep the default input data path in the configfile, as it points to the data directory in the respository.\n",
    "```ini\n",
    "[data_paths]\n",
    "...\n",
    "input_path = ${local_path}/data\n",
    "```\n",
    "\n",
    "You will need to specifiy a location on disk for the output data:\n",
    "```ini\n",
    "[data_paths]\n",
    "...\n",
    "output_path = /Users/MyUser/outputs\n",
    "```\n",
    "\n",
    "And if you like, you can change the experiment suffix that will be attached to the filename of your output folder:\n",
    "```ini\n",
    "[data_paths]\n",
    "...\n",
    "run_name = notebook_test\n",
    "```\n",
    "\n",
    "In the end the `[data_paths]` section in the configfile should look something like this:\n",
    "```ini\n",
    "[data_paths]\n",
    "local_path = /Users/MyUser/repos/ExoTiC-ISM\n",
    "input_path = ${local_path}/data\n",
    "output_path = /Users/MyUser/outputs\n",
    "run_name = notebook_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup\n",
    "\n",
    "For the general setup, we are interested in what instrument and grating was used to collect the data on which specific stellar system. This is done in the configfile section `[setup]`.\n",
    "\n",
    "First, define what star the data is from - the example data is of Wasp-17b, so we dubbed the stellar system section `W17`:\n",
    "```ini\n",
    "[setup]\n",
    "data_set = W17\n",
    "```\n",
    "\n",
    "The instrument for this data is Wide Field Camera 3 (WFC3) and the data was taken with the IR grating G141:\n",
    "```ini\n",
    "[setup]\n",
    "...\n",
    "instrument = WFC3\n",
    "grating = G141\n",
    "```\n",
    "\n",
    "You need to decide which general grid of systematic models you want to use. For WFC3, this grid consists of 50 distinct systematic models used to fit the data, in all of which the transit depth `rl` and the baseline flux `flux0` are always free parameters. The choice you have to make is which of the variable parameters **epoch**, **inclination**, **MsMpR** and **eccentricity** you want to keep fixed (\"frozen\") or free (\"thawed\"):\n",
    "\n",
    "- `fix_time`: all of them are frozen\n",
    "- `fit_time`: epoch is thawed, other three are frozen\n",
    "- `fit_inclin`: inclination is thawed, other three are frozen\n",
    "- `fit_msmpr`: MsMpR is thawed, other three are frozen\n",
    "- `fit_ecc`: eccentricity is thawed, other three are frozen\n",
    "- `fit_all`: eccentricity is frozen, other three are thawed\n",
    "\n",
    "```ini\n",
    "[setup]\n",
    "...\n",
    "grid_selection = fit_time\n",
    "```\n",
    "\n",
    "The repository provides a range of both 1D and 3D limb darkening models, so you need to chose which ones to use:\n",
    "```ini\n",
    "[setup]\n",
    "...\n",
    "ld_model = 3D\n",
    "```\n",
    "\n",
    "And finally, you need to decide whether you want the fit results of each systematic model to be displayed at runtime, which can be helpful for troubleshooting but also annoying when trying to work while the code is running, and whether you want the PDF report with the results to be created. It is highly recommended to always keep this `True`, we included the feature to disable this purely to be able to skip this step if the required python packages for this are not available for any reason.\n",
    "\n",
    "```ini\n",
    "[setup]\n",
    "...\n",
    "plotting = True\n",
    "report = True\n",
    "```\n",
    "\n",
    "In the end, your general setup should look something like this:\n",
    "```ini\n",
    "[setup]\n",
    "data_set = W17\n",
    "instrument = WFC3\n",
    "grating = G141\n",
    "grid_selection = fit_time\n",
    "ld_model = 3D\n",
    "plotting = True\n",
    "report = True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth model parameters\n",
    "\n",
    "At convenient points in the code, the marginalisation script will calculate a smooth model of the fit. The following configfile section defines at what resultion it gets calculated and what its range in terms of planet phase it will encapsulate. You might need to adjust especially the `half_range` if your data has a significantly different transit length that the example data. For `W17` we use:\n",
    "\n",
    "```ini\n",
    "[smooth_model]\n",
    "resolution = 0.0001\n",
    "half_range = 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stellar and planetary system parameters\n",
    "\n",
    "The last important bit is to define the input parameters for the star and planet. For each new data set you want to analyse, you need to create a new section in your configfile that carries the same section name like what you put in the `data_set` name in `[section] -> data_set`. For the example data, we have a section called `[W17]` that holds all stellar parameters (for limb darkening) and the initial guesses for the fitting parameters for the planetary transit.\n",
    "\n",
    "```ini\n",
    "[W17]\n",
    "...\n",
    "rl = 0.12169232\n",
    "epoch = 57957.970153390\n",
    "inclin = 87.34635\n",
    "ecc = 0.0\n",
    "omega = 0.0\n",
    "Per = 3.73548535\n",
    "aor = 7.0780354\n",
    "\n",
    "; limb darkening parameters\n",
    "metallicity = -1.0\n",
    "Teff = 6550\n",
    "logg = 4.5\n",
    "```\n",
    "\n",
    "At the top of this section is where you drop in the file names of your data files, one lightcurve file and one file with a wavelength array. If you have more than one data set available for a specific system, your filenames should indicate by what they differ, e.g. different gratings (e.g. G141 or G102), so that you have to make only one single change in your configfile when your changing from one data set to the other. For example, if you have W17 data both with the G141 as well as the G102 grating, you can include the grating name in the filenames and adjust them in the configfile in such a way that the correct files get picked when you adjust your grating of choice in `[setup] -> grating`.\n",
    "\n",
    "```ini\n",
    "[setup]\n",
    "...\n",
    "grating = G141\n",
    "...\n",
    "\n",
    "[W17]\n",
    "lightcurve_file = W17_${setup:grating}_lightcurve_test_data.txt\n",
    "wvln_file = W17_${setup:grating}_wavelength_test_data.txt\n",
    "```\n",
    "\n",
    "which represents these data files:  \n",
    "```bash\n",
    "W17_G141_lightcurve_test_data.txt\n",
    "W17_G141_wavelength_test_data.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting with Sherpa\n",
    "\n",
    "Before turning to the full marginalisation code, let us have a look at how the fitting is performed. We use a package called `sherpa` (https://sherpa.readthedocs.io/en/latest/). `Sherpa` allows you to write your own model that subsequently gets fit to your data with a **statistic** and an **optimizer** of your choice. ExoTiC-ISM uses the **chi-squared** statistic and the **Levenberg-Marquardt** optimizer, which you can read more about in [Wakeford et al. (2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...819...10W/abstract) and in the `sherpa` documentation, as well as in any standard sources for least-squares optimisations.\n",
    "\n",
    "We have transfered our transit model, which is the analytic transit model from [Mandel & Agol (2002)](https://ui.adsabs.harvard.edu/abs/2002ApJ...580L.171M/abstract) into the `sherpa` model format. To make sure this works, we can instantiate a simple transit model based on a simple data set that we will create here.\n",
    "\n",
    "Just as for a real data set, we created a configfile section for our custom transit, containing the initial guess for the fitting parameters as well as the limb darkening parameters of the star.\n",
    "\n",
    "```ini\n",
    "[simple_transit]\n",
    "rl = 0.1\n",
    "epoch = 0.\n",
    "inclin = 90\n",
    "ecc = 0.0\n",
    "omega = 0.0\n",
    "Per = 3.5\n",
    "aor = 7.0\n",
    "\n",
    "; limb darkening parameters\n",
    "metallicity = 0.0\n",
    "Teff = 5500\n",
    "logg = 4.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.constants import G\n",
    "\n",
    "os.chdir('../../exotic-ism')\n",
    "import margmodule as marg\n",
    "from config import CONFIG_INI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple phase array, grouped in three observation sets\n",
    "data_x = np.array([-0.046, -0.044, -0.042, -0.040, -0.038, -0.036, -0.034,\n",
    "                   -0.032, -0.030, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004,\n",
    "                   0.006, 0.008, 0.01, 0.032, 0.034, 0.036, 0.038, 0.040,\n",
    "                   0.042, 0.044, 0.046,0.048])\n",
    "\n",
    "# Create simple light curve\n",
    "data_y = np.array([1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000,\n",
    "                   1.0000000, 1.0000000, 1.0000000, 1.0000000, 0.99000000,\n",
    "                   0.99000000, 0.99000000, 0.99000000, 0.99000000, 0.99000000,\n",
    "                   0.99000000, 0.99000000, 0.99000000, 1.0000000, 1.0000000,\n",
    "                   1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000,\n",
    "                   1.0000000, 1.0000000])\n",
    "\n",
    "# Assume each data point has exactly the same uncertainty (error)\n",
    "uncertainty = np.array([0.0004] * len(data_x))\n",
    "\n",
    "# Add random scatter to the light curve\n",
    "random_scatter = np.array([0.32558253, -0.55610514, -1.1150768, -1.2337022, -1.2678875,\n",
    "                           0.60321692, 1.1025507, 1.5080730, 0.76113001, 0.51978011,\n",
    "                           0.72241364, -0.086782108, -0.22698337, 0.22780245, 0.47119014,\n",
    "                           -2.1660677, -1.2477670, 0.28568456, 0.40292731, 0.077955817,\n",
    "                           -1.1090623, 0.66895172, -0.59215439, 0.79973968, 1.0603756,\n",
    "                           0.82684954, -1.8334587])\n",
    "\n",
    "data_y += random_scatter * uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a quick look at our created data with `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Simple data')\n",
    "plt.errorbar(data_x, data_y, yerr=uncertainty, fmt='.')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use `sherpa`, we need to create a `sherpa` data object from our simple data, which has its own methods to plot the data. This is exactly the same data we are just showing a different way to plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sherpa.data import Data1D\n",
    "from sherpa.plot import DataPlot\n",
    "\n",
    "data = Data1D('simple_transit', data_x, data_y, staterror=uncertainty)   # create data object\n",
    "dplot = DataPlot()         # create data *plot* object\n",
    "dplot.prepare(data)       # prepare plot\n",
    "dplot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate the transit model, we need to read some of the parameters from the configfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transit parameters\n",
    "planet_sys = CONFIG_INI.get('setup', 'data_set')\n",
    "dtosec = CONFIG_INI.getfloat('constants', 'dtosec')\n",
    "period = CONFIG_INI.getfloat(planet_sys, 'Per')\n",
    "Per = period * dtosec\n",
    "aor = CONFIG_INI.getfloat(planet_sys, 'aor')\n",
    "constant1 = (G * Per * Per / (4 *np.pi * np.pi))**(1/3)\n",
    "msmpr = (aor/(constant1))**3\n",
    "\n",
    "print('msmpr: {}'.format(msmpr))\n",
    "print('G: {}'.format(G.value))\n",
    "print('Per: {} sec'.format(Per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will provide limb darkening coefficients that make sense. In the main marginalization script, these get calculated based on the configfile inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limb darkening coefficients\n",
    "c1 = 0.66396105\n",
    "c2 = -0.12617095\n",
    "c3 = 0.053649047\n",
    "c4 = -0.026713433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and instantiate a transit object wit our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = marg.Transit(data_x[0], msmpr, c1, c2, c3, c4, flux0=data_y[0], x_in_phase=True, name='transit_model', sh=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily check which fitting parameters are currently thawed and whcih are frozen, and what their initial guesses are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, we will freeze almost all parameters except for the tranist depth `rl` and the baseline flux `flux0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freese almost all parameters\n",
    "model.epoch.freeze()\n",
    "model.inclin.freeze()\n",
    "model.msmpr.freeze()\n",
    "model.ecc.freeze()\n",
    "model.m_fac.freeze()\n",
    "model.hstp1.freeze()\n",
    "model.hstp2.freeze()\n",
    "model.hstp3.freeze()\n",
    "model.hstp4.freeze()\n",
    "model.xshift1.freeze()\n",
    "model.xshift2.freeze()\n",
    "model.xshift3.freeze()\n",
    "model.xshift4.freeze()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the model, we need to calculate it on a grid of x-values. To do so, will will create a smooth and uniform x-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_smooth = np.arange(data_x[0], data_x[-1], CONFIG_INI.getfloat('smooth_model', 'resolution'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the model on this smooth grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First dump all model parameters into tuple to access them easier\n",
    "# (there is an easier way for this which we will implement at a later point)\n",
    "params = (model.rl.val, model.flux0.val, model.epoch.val, model.inclin.val, model.MsMpR.val,\n",
    "          model.ecc.val, model.omega.val, model.period.val, model.tzero.val, model.c1.val,\n",
    "          model.c2.val, model.c3.val, model.c4.val, model.m_fac.val, model.hstp1.val,\n",
    "          model.hstp2.val, model.hstp3.val, model.hstp4.val, model.xshift1.val,\n",
    "          model.xshift2.val, model.xshift3.val, model.xshift4.val)\n",
    "\n",
    "# Calculate model on denser grid to display\n",
    "y_smooth_model = model.calc(pars=params, x=x_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And display that\n",
    "plt.plot(x_smooth, y_smooth_model, c='orange')\n",
    "plt.errorbar(data_x, data_y, yerr=uncertainty, fmt='.')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Flux')\n",
    "plt.title('Smooth model over simple data, before fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how the current model parameters are not a good fit to the data; especially the transit depth is way off. In order to make this better, we will now chose our statistic and an optimizer, and perform a fit.\n",
    "\n",
    "Same as in the marginalisatino script, we will chose the chi-squared statistic and an LM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sherpa.optmethods import LevMar\n",
    "from sherpa.stats import Chi2\n",
    "\n",
    "# Set up the statistic and optimizer\n",
    "stat = Chi2()\n",
    "opt = LevMar()\n",
    "opt.config['epsfcn'] = np.finfo(float).eps   # adjusting epsfcn to double precision\n",
    "\n",
    "print(stat)\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined a data object and put that into a model object. We have settled on a statistic and an optimizer, so now we will combine the statistic and optimizer with the data and model to instantiate a fit object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sherpa.fit import Fit\n",
    "\n",
    "tfit = Fit(data, model, stat=stat, method=opt)\n",
    "print(tfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is left to do is to perform the fit and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitresult = tfit.fit()\n",
    "print(fitresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the covariance matrix of the fit directly, from where we calculate the parameter errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian = np.sqrt(fitresult.extra_output['covar'].diagonal())\n",
    "rl_err = hessian[0]\n",
    "\n",
    "print('rl = {} +/- {}'.format(model.rl.val, rl_err))\n",
    "print('Reduced chi-squared: {}'.format(fitresult.rstat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we replot the new fit, we can see how this looks way better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump parameters in a single tuple again\n",
    "params_result = (model.rl.val, model.flux0.val, model.epoch.val, model.inclin.val, model.MsMpR.val,\n",
    "                 model.ecc.val, model.omega.val, model.period.val, model.tzero.val, model.c1.val,\n",
    "                 model.c2.val, model.c3.val, model.c4.val, model.m_fac.val, model.hstp1.val,\n",
    "                 model.hstp2.val, model.hstp3.val, model.hstp4.val, model.xshift1.val,\n",
    "                 model.xshift2.val, model.xshift3.val, model.xshift4.val)\n",
    "\n",
    "# Recalculate model on smooth grid now that we performed the fit\n",
    "y_smooth_fit = model.calc(pars=params_result, x=x_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit\n",
    "plt.plot(x_smooth, y_smooth_fit, c='orange')\n",
    "plt.errorbar(data_x, data_y, yerr=uncertainty, fmt='.')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Flux')\n",
    "plt.title('Smooth model over simple data, after fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limb darkening\n",
    "\n",
    "The limb darkening coefficients used for the fitting get calculated in the main marginalisation script, and here we will briefly look into how this is done.\n",
    "\n",
    "First, we usually read the limb darkening parameters of the stellar system in question from the configfile. In this tutorial, we set some of them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which stellar and planetary system are we working on?\n",
    "exoplanet = 'simple_transit'\n",
    "print('System: {}'.format(exoplanet))\n",
    "\n",
    "# Read its limb darkening parameters\n",
    "M_H = CONFIG_INI.getfloat(exoplanet, 'metallicity')    # stellar metallicity - limited ranges available\n",
    "Teff = CONFIG_INI.getfloat(exoplanet, 'Teff')   # stellar temperature - for 1D models: steps of 250 starting at 3500 and ending at 6500\n",
    "logg = CONFIG_INI.getfloat(exoplanet, 'logg')   # log(g), stellar gravity - depends on whether 1D or 3D limb darkening models are used\n",
    "print('M_H: {}'.format(M_H))\n",
    "print('Teff: {}'.format(Teff))\n",
    "print('logg: {}'.format(logg))\n",
    "\n",
    "# Define limb darkening directory, which is inside this package\n",
    "# and read limb darkening model choice and grating\n",
    "limbDir = os.path.join('..', 'Limb-darkening')\n",
    "ld_model = '1D'\n",
    "grat = 'G141'\n",
    "print('LD model: {}'.format(ld_model))\n",
    "print('Grating: {}'.format(grat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to calculate the limb darkening coefficients. To do that, we need to load the wavelength array, wchich we will do for W17, on whichever grid is currently picked in the configfile.  \n",
    "In the main script, the four non-linear limb darkening coefficients get used: c1, c2, c3 and c4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limb_darkening import limb_dark_fit\n",
    "\n",
    "# Load wavelength array\n",
    "dataDir = os.path.join(CONFIG_INI.get('data_paths', 'input_path'), 'W17')\n",
    "get_wvln = CONFIG_INI.get('W17', 'wvln_file')\n",
    "wavelength = np.loadtxt(os.path.join(dataDir, get_wvln), skiprows=3)\n",
    "\n",
    "_uLD, c1, c2, c3, c4, _cp1, _cp2, _cp3, _cp4, _aLD, _bLD = limb_dark_fit(grat, wavelength, M_H, Teff,\n",
    "                                                                         logg, limbDir, ld_model)\n",
    "\n",
    "print('\\nc1 = {}'.format(c1))\n",
    "print('c2 = {}'.format(c2))\n",
    "print('c3 = {}'.format(c3))\n",
    "print('c4 = {}'.format(c4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full marginalisation\n",
    "\n",
    "To launch the full marginalization over all systematic models, **run `marginalization.py`**. This script first performs a fit on all systematic models to scale the uncertainties to unity chi-squared, then it fits all systematic models again and saves the fit parameters for each of them. In a final step, the marginalization over all stsyematic models is performed, plotted and saved to `[data_paths] -> output_path`.\n",
    "\n",
    "The reson we fit all models twice is the error estimation. The input flux errors are pure photon noise, which does not incorporate all information on real noise sources. The first fit is intended to understant additional noise sources. This is done by rescaling all uncertainties so that the data and the model have a reduced chi-squared of one. This means that the uncertainties will get slightly larger than pur photon noise and we use these more conservative (and probably more accurate) errors when running the second round of fitting, yielding fit parameters that we can trust better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
